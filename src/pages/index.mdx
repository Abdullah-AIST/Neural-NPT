---
layout: ../layouts/Layout.astro
title: "Neural-NPT: A Reinforcement Learning Perspective to Dynamic Non-Prehensile Object Transportation"
description: A project page for Neural-NPT
favicon: favicon.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import ThreeColumns from "../components/ThreeColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import outside from "../assets/Supplementary Video.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"


import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Author one",
      //url: "https://roman.technology",
      institution: "Independent",
      notes: ["*", "†"],
    },
    {
      name: "Author Two",
      institution: "Institution Two",
      notes: ["*", "†"],
    },
    {
      name: "Author Three",
      institution: "Institution Three",
      notes: ["†"],
    },
    {
      name: "Author Four",
      institution: "Institution Four",
    },
  ]}
  conference="Robotics and Automation Letters (RAL) 2025"
  notes={[
    {
      symbol: "*",
      text: "author note one",
    },
    {
      symbol: "†",
      text: "author note two",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://abdullah-aist.github.io/Neural-NPT/",
      icon: "ri:github-line",
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    }
  ]}
  />

<Video source={outside} />

<HighlightedSection>

## Abstract

This work proposes a learning-based approach to dynamic non-prehensile object transportation (e.g., "Waiter's Problem"). Model-based approaches start with a strong dynamic grasping assumption and optimize trajectories under such constraints. However, this optimization process is prone to failure, slow convergence, or suboptimal solutions due to the constraints imposed by dynamic grasping. To address these limitations, we propose a motion-planning neural policy learned via reinforcement learning. The task is formulated as a Markov decision problem, with carefully designed observation space, action space, and reward formulation. Acceleration-based action was crucial for smooth trajectory generation and sim2real transferability. Prior to training, the sim2real robot dynamics gap was minimized through system identification. Towards generalization to wide range of objects, we randomized object size, friction, mass, center of mass, and initial pose during training. By curating the randomization process, different policies are obtained including an "Optimal" policy with full observability and a "Robust" policy under partial observability of object inertia and pose. The trained policy is used for offline trajectory planning, resulting in fast and robust trajectories successfully deployed in both simulated and physical environments. Both our "Optimal" and "Robust" policies had higher peak velocities and accelerations, and shorter reaching times compared to the model-based baseline.

</HighlightedSection>

# Real-world Validation

## Transport Anything
Randomly swapping objects during exection of Optimal policy optimized for the WoodBlock. Learned trajectories is stable under arbitrary objects. 
<Figure>
  <YouTubeVideo slot="figure" videoId="MJLgvKNcebw" />
  <span slot="caption">Optimal Policy [32/32]</span>
</Figure>
## Optimal Vs. Optimal-DG 

Optimal is faster but may not be as stable for long time operation

### WoodBlock
<ThreeColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="v964eNtG6V8" />
    <span slot="caption">Optimal Policy [32/32]</span>
  </Figure>
  <Figure slot="center">
    <YouTubeVideo slot="figure" videoId="sYLbvGEPlGY" />
    <span slot="caption">Optimal-DG Policy [32/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="rvBT0EBpbfs" />
    <span slot="caption">Upright Baseline [29/32]</span>
  </Figure>
</ThreeColumns>

### Chips
<ThreeColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="WJdUwITaY34" />
    <span slot="caption">Optimal Policy [32/32]</span>
  </Figure>
  <Figure slot="center">
    <YouTubeVideo slot="figure" videoId="ITR15S27lao" />
    <span slot="caption">Optimal-DG Policy [32/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="34y-fJQ8hoc" />
    <span slot="caption">Upright Baseline [31/32]</span>
  </Figure>
</ThreeColumns>

### CrackerBox
<ThreeColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="6jBM9eprjSg" />
    <span slot="caption">Optimal Policy [32/32]</span>
  </Figure>
  <Figure slot="center">
    <YouTubeVideo slot="figure" videoId="VqgffYWCYBQ" />
    <span slot="caption">Optimal-DG Policy [32/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="1G9t_jd6FXI" />
    <span slot="caption">Upright Baseline [28/32]</span>
  </Figure>
</ThreeColumns>

### BleachCleanser
<ThreeColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="c6xx1Tetu4M" />
    <span slot="caption">Optimal Policy [12/32]</span>
  </Figure>
  <Figure slot="center">
    <YouTubeVideo slot="figure" videoId="jUCgVkvo1g8" />
    <span slot="caption">Optimal-DG Policy [22/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="cArGAAFWHBw" />
    <span slot="caption">Upright Baseline [30/32]</span>
  </Figure>
</ThreeColumns>

### TallBox-Center
<ThreeColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="qDl7Of0KIs8" />
    <span slot="caption">Optimal Policy [14/32]</span>
  </Figure>
  <Figure slot="center">
    <YouTubeVideo slot="figure" videoId="u-PB8xAhsdI" />
    <span slot="caption">Optimal-DG Policy [32/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="lpckEQ3DGn4" />
    <span slot="caption">Upright Baseline [24/32]</span>
  </Figure>
</ThreeColumns>

## COM-Robust Vs. Optimal

For objects with inertia uncertainity, COM-Robust policy is more stable yet slower.

### PowerDrill
<ThreeColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="ATX4jsaAYhs" />
    <span slot="caption">COM-Robust Policy [14/32]</span>
  </Figure>
  <Figure slot="center">
    <YouTubeVideo slot="figure" videoId="RoY0br8Qn28" />
    <span slot="caption">Optimal Policy [0/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="Hte49KuQmUE" />
    <span slot="caption">Upright-Robust Baseline [*/32]</span>
  </Figure>
</ThreeColumns>

### Pitcher
<ThreeColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="1mF-UcuJ4vY" />
    <span slot="caption">COM-Robust Policy [25/32]</span>
  </Figure>
  <Figure slot="center">
    <YouTubeVideo slot="figure" videoId="mVLbxQnohHo" />
    <span slot="caption">Optimal Policy [2/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="UW1CwZUFwXo" />
    <span slot="caption">Upright-Robust Baseline [*/32]</span>
  </Figure>
</ThreeColumns>

### TallBox-Top
<ThreeColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="15A-d8hpFxQ" />
    <span slot="caption">COM-Robust Policy [23/32]</span>
  </Figure>
  <Figure slot="center">
    <YouTubeVideo slot="figure" videoId="69DSX8XYZVg" />
    <span slot="caption">Optimal Policy [0/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="OpuBcsAeLQk" />
    <span slot="caption">Upright-Robust Baseline [*/32]</span>
  </Figure>
</ThreeColumns>

## Multi-Robust Vs. Optimal

For objects with pose uncertainity (i.e. Multi-object transportation), Multi-Robust policy is more stable yet slower.

### Noodles
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="69lIC0Mm9yQ" />
    <span slot="caption">Multi-Robust Policy [32/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="fTvm-CGpfos" />
    <span slot="caption">Optimal Policy [5/32]</span>
  </Figure>
</TwoColumns>

### Bottles
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="HMh7PN9bx_Q" />
    <span slot="caption">Multi-Robust Policy [32/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="fn8P6RxZKU4" />
    <span slot="caption">Optimal Policy [0/32]</span>
  </Figure>
</TwoColumns>


## Friction Coefficient Variations

We test performance of Optimal policy for two surfaces rubber (<LaTeX inline formula="\mu=0.8" />) and paper (<LaTeX inline formula="\mu=0.4" />). Different trajectories are generated by varying the input friction to the policy. It is always safer to assume low friction (<LaTeX inline formula="\mu=0.1" />). High-friction surfaces fails due to tilting, while low-friction surfaces failed to slippage.
### (<LaTeX inline formula="\mu=0.1" />)
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="S1WnOpCSgmU" />
    <span slot="caption"> Rubber [32/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="_Ox-iBJwflI" />
    <span slot="caption"> Paper [13/32]</span>
  </Figure>
</TwoColumns>

### (<LaTeX inline formula="\mu=0.2" />)
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="wuPYgTjuEGg" />
    <span slot="caption"> Rubber [32/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="Bq4xXNTHGDU" />
    <span slot="caption"> Paper [4/32]</span>
  </Figure>
</TwoColumns>

### (<LaTeX inline formula="\mu=0.3" />)
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="DXm9O0D05DA" />
    <span slot="caption"> Rubber [29/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="B5pcR3HI5Yg" />
    <span slot="caption"> Paper [2/32]</span>
  </Figure>
</TwoColumns>

### (<LaTeX inline formula="\mu=0.5" />)
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="ldP0EEFQokY" />
    <span slot="caption"> Rubber [5/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="HXJdGW9n2M0" />
    <span slot="caption"> Paper [1/32]</span>
  </Figure>
</TwoColumns>


## Extra Footage
### NoodlesTower
Multiple stacked objects 
<Figure>
  <YouTubeVideo slot="figure" videoId="9VUIa_-5jJM" />
  <span slot="caption">Multi-Robust Policy [8/32]</span>
</Figure>

### CrackerBox-Rotated
Extra air resistance.
<Figure>
  <YouTubeVideo slot="figure" videoId="GYYoitFCKG0" />
  <span slot="caption">Optimal Policy [32/32]</span>
</Figure>


## BibTeX citation

```bibtex
@misc{neuralNPT_ral2025,
  author = "{Author One}",
  title = "Neural-NPT: A Reinforcement Learning Perspective to Dynamic Non-Prehensile Object Transportation",
  year = "2025",
}
```