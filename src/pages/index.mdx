---
layout: ../layouts/Layout.astro
title: "Neural-NPT: A Reinforcement Learning Perspective to Dynamic Non-Prehensile Object Transportation"
description: A project page for Neural-NPT
favicon: favicon.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"


import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Author one",
      //url: "https://roman.technology",
      institution: "Independent",
      notes: ["*", "†"],
    },
    {
      name: "Author Two",
      institution: "Institution Two",
      notes: ["*", "†"],
    },
    {
      name: "Author Three",
      institution: "Institution Three",
      notes: ["†"],
    },
    {
      name: "Author Four",
      institution: "Institution Four",
    },
  ]}
  conference="Robotics and Automation Letters (RAL) 2025"
  notes={[
    {
      symbol: "*",
      text: "author note one",
    },
    {
      symbol: "†",
      text: "author note two",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://abdullah-aist.github.io/Neural-NPT/",
      icon: "ri:github-line",
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    }
  ]}
  />

{/* <Video source={outside} /> */}

<HighlightedSection>

## Abstract

This work proposes a learning-based approach to dynamic non-prehensile object transportation (e.g., "Waiter's Problem"). Model-based approaches start with a strong dynamic grasping assumption and optimize trajectories under such constraints. However, this optimization process is prone to failure, slow convergence, or suboptimal solutions due to the constraints imposed by dynamic grasping. To address these limitations, we propose a motion-planning neural policy learned via reinforcement learning. The task is formulated as a Markov decision problem, with carefully designed observation space, action space, and reward formulation. Acceleration-based action was crucial for smooth trajectory generation and sim2real transferability. Prior to training, the sim2real robot dynamics gap was minimized through system identification. Towards generalization to wide range of objects, we randomized object size, friction, mass, center of mass, and initial pose during training. By curating the randomization process, different policies are obtained including an "Optimal" policy with full observability and a "Robust" policy under partial observability of object inertia and pose. The trained policy is used for offline trajectory planning, resulting in fast and robust trajectories successfully deployed in both simulated and physical environments. Both our "Optimal" and "Robust" policies had higher peak velocities and accelerations, and shorter reaching times compared to the model-based baseline.

</HighlightedSection>

# Real-world Validation
fdsafdsfsf

## Optimal Vs. Optimal-DG 

Optimal is faster but may not be as stable for long time operation

### WoodBlock
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="v964eNtG6V8" />
    <span slot="caption">Optimal Policy [32/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="sYLbvGEPlGY" />
    <span slot="caption">Optimal-DG Policy [32/32]</span>
  </Figure>
</TwoColumns>

### Chips
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="WJdUwITaY34" />
    <span slot="caption">Optimal Policy [32/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="ITR15S27lao" />
    <span slot="caption">Optimal-DG Policy [32/32]</span>
  </Figure>
</TwoColumns>

### CrackerBox
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="6jBM9eprjSg" />
    <span slot="caption">Optimal Policy [32/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="VqgffYWCYBQ" />
    <span slot="caption">Optimal-DG Policy [32/32]</span>
  </Figure>
</TwoColumns>

### BleachCleanser
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="c6xx1Tetu4M" />
    <span slot="caption">Optimal Policy [12/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="jUCgVkvo1g8" />
    <span slot="caption">Optimal-DG Policy [22/32]</span>
  </Figure>
</TwoColumns>

### TallBox-Center
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="qDl7Of0KIs8" />
    <span slot="caption">Optimal Policy [14/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="u-PB8xAhsdI" />
    <span slot="caption">Optimal-DG Policy [32/32]</span>
  </Figure>
</TwoColumns>

## COM-Robust Vs. Optimal

For objects with inertia uncertainity, COM-Robust policy is more stable yet slower.

### PowerDrill
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="ATX4jsaAYhs" />
    <span slot="caption">COM-Robust Policy [14/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="RoY0br8Qn28" />
    <span slot="caption">Optimal Policy [0/32]</span>
  </Figure>
</TwoColumns>

### Pitcher
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="1mF-UcuJ4vY" />
    <span slot="caption">COM-Robust Policy [25/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="mVLbxQnohHo" />
    <span slot="caption">Optimal Policy [2/32]</span>
  </Figure>
</TwoColumns>

### TallBox-Top
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="15A-d8hpFxQ" />
    <span slot="caption">COM-Robust Policy [23/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="69DSX8XYZVg" />
    <span slot="caption">Optimal Policy [0/32]</span>
  </Figure>
</TwoColumns>

## Multi-Robust Vs. Optimal

For objects with inertia uncertainity, COM-Robust policy is more stable yet slower.

### Noodles
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="69lIC0Mm9yQ" />
    <span slot="caption">Multi-Robust Policy [14/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="fTvm-CGpfos" />
    <span slot="caption">Optimal Policy [0/32]</span>
  </Figure>
</TwoColumns>

### Bottles
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="HMh7PN9bx_Q" />
    <span slot="caption">Multi-Robust Policy [14/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="fn8P6RxZKU4" />
    <span slot="caption">Optimal Policy [0/32]</span>
  </Figure>
</TwoColumns>


## Extra Footage

For objects with inertia uncertainity, COM-Robust policy is more stable yet slower.

### NoodlesTower
Multiple stacked objects 
<Figure>
  <YouTubeVideo slot="figure" videoId="9VUIa_-5jJM" />
  <span slot="caption">Multi-Robust Policy [8/32]</span>
</Figure>

### CrackerBox-Rotated
Extra air resistance.
<Figure>
  <YouTubeVideo slot="figure" videoId="GYYoitFCKG0" />
  <span slot="caption">Optimal Policy [32/32]</span>
</Figure>

### Faster WoodBlock
<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="TDm0R-4XJ34" />
    <span slot="caption">Optimal Policy (<LaTeX inline formula="\mu=0.3" />) [16/32]</span>
  </Figure>
  <Figure slot="right">
    <YouTubeVideo slot="figure" videoId="N8abYRtAokM" />
    <span slot="caption">Optimal Policy (<LaTeX inline formula="\mu=0.5" />) [4/32]</span>
  </Figure>
</TwoColumns>


## LaTeX

You can also add LaTeX formulas, rendered during the build process using [KaTeX](https://katex.org/) so they're quick to load for visitors of your project page. You can write them inline, like this: <LaTeX inline formula="a^2 + b^2 = c^2" />. Or, you can write them as a block:

<LaTeX formula="\int_a^b f(x) dx" />

## Tables

You can add simple tables using [GitHub Flavored Markdown syntax](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/organizing-information-with-tables):

| Model | Accuracy | F1 score | Training time (hours) |
| :--- | :---: | :---: | :---: |
| BERT-base | 0.89 | 0.87 | 4.5 |
| RoBERTa-large | 0.92 | 0.91 | 7.2 |
| DistilBERT | 0.86 | 0.84 | 2.1 |
| XLNet | 0.90 | 0.89 | 6.8 |

## BibTeX citation

```bibtex
@misc{neuralNPT_ral2025,
  author = "{Author One}",
  title = "Neural-NPT: A Reinforcement Learning Perspective to Dynamic Non-Prehensile Object Transportation",
  year = "2025",
}
```